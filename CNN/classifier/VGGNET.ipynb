{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf16917f",
   "metadata": {},
   "source": [
    "## VGGNET 모델 파이토치 구현\n",
    "### 더 3x3 conv 필터를 사용하여 더 적은 파라미터로 같은 receptive field를 갖음 - > 자세한 내용은 논문\n",
    "### 논문 링크: https://arxiv.org/abs/1409.1556"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea459861",
   "metadata": {},
   "source": [
    "## 데이터셋 다운로드 훈련용 50만장, 검증용 10만장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install natsort opencv-python tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdc139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import cv2\n",
    "import torchvision.transforms as T\n",
    "import natsort\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a381286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대규모의 데이터로 학습 하고 싶으면 주석해제\n",
    "\n",
    "# torchvision.datasets.INaturalist(root='./data',version='2021_train_mini', download=True)\n",
    "# torchvision.datasets.INaturalist(root='./data',version='2021_valid', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ac61b",
   "metadata": {},
   "source": [
    "## 데이터의 클래스 수가 10000개여서 300개에 대해서만 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class INaturalist_Dataset(Dataset):\n",
    "    def __init__(self, train='train', transforms=None):\n",
    "        self.root_path = 'data/2021_train_mini' if train=='train' else 'data/2021_valid'\n",
    "      \n",
    "        self.class_names = [class_name for class_name in natsort.natsorted(os.listdir(self.root_path))][:300] \n",
    "        self.data_path = []\n",
    "        for class_idx, class_name in enumerate(self.class_names):\n",
    "            for file in os.listdir(os.path.join(self.root_path, class_name)):\n",
    "                self.data_path.append({'file_path':os.path.join(self.root_path, class_name, file), 'class_idx':class_idx})\n",
    "        self.transforms = transforms\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.data_path[idx]['file_path']\n",
    "        class_idx =  self.data_path[idx]['class_idx']\n",
    "        img = cv2.imread(file_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img, class_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13565ad0",
   "metadata": {},
   "source": [
    "## 데이터증강 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c1e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "train_transform = T.Compose([\n",
    "    #T.ToPILImage(),\n",
    "    T.ToTensor(),\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    #T.ToPILImage(),\n",
    "    T.ToTensor(),\n",
    "    T.Resize((224, 224)),\n",
    "   \n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "# 대규모 데이터 학습 하고 싶으면 해제\n",
    "# trainset = INaturalist_Dataset('train', train_transform)\n",
    "# testset = INaturalist_Dataset('val', val_transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "#                                           shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "#                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237b986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.STL10('./data', split='train', download=True, transform=train_transform)\n",
    "testset = torchvision.datasets.STL10('./data', split='test', download=True, transform=val_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b227d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 이미지를 보여주기 위한 함수\n",
    "classes = [class_name.split('_')[-2]+'_'+class_name.split('_')[-1] \n",
    "           for class_name in natsort.natsorted(os.listdir('data/2021_train_mini'))[:300]]\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 학습용 이미지를 무작위로 가져오기\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# 이미지 보여주기\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# 정답(label) 출력\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af2eaa",
   "metadata": {},
   "source": [
    "## VGGNET 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, features,output_dim,init_weights=True):\n",
    "        super().__init__()        \n",
    "        self.features = features  # 이미지 특징 추출       \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(7)        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, output_dim),\n",
    "        )\n",
    "        # 초기 가중치 초기화\n",
    "        if init_weights:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.normal_(m.weight, 0, 0.01)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        h = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(h)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e1e76",
   "metadata": {},
   "source": [
    "## vggNet layer의 개수에 따른 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_config = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "vgg13_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "vgg16_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, \n",
    "                512, 'M']\n",
    "\n",
    "vgg19_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
    "                512, 512, 512, 512, 'M']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f953876",
   "metadata": {},
   "source": [
    "## 특징 학습 layer 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2945152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_layers(config, batch_norm):    \n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    \n",
    "    for c in config:\n",
    "        assert c == 'M' or isinstance(c, int)\n",
    "        if c == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size = 2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, c, kernel_size = 3, padding = 1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace = True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace = True)]\n",
    "            in_channels = c\n",
    "            \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_layers = get_vgg_layers(vgg19_config, batch_norm = True) #batch_norm 배치 단위로 입력을 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9b411",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb233de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = VGG(vgg19_layers, 300).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 30\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702f0db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78327cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, model,optimizer, criterion):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.train()\n",
    "    for i, data in enumerate(tqdm(loader), 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "       \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss /len(loader)\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(loader, model, optimizer, criterion):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(loader), 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "    epoch_loss = running_loss /len(loader)\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ac457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    best_acc = 0\n",
    "\n",
    "    train_loss, train_acc = train(trainloader,model, optimizer, criterion)\n",
    "    val_loss, val_acc = validation(testloader,model, optimizer, criterion)\n",
    "    scheduler.step()\n",
    "    is_best = val_acc > best_acc\n",
    "    best_acc1 = max(val_acc, best_acc)\n",
    "    \n",
    "    \n",
    "    save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc1': best_acc,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'scheduler' : scheduler.state_dict()\n",
    "        }, is_best)\n",
    "        \n",
    "    print(f'[{epoch}], train_loss:{train_loss:.4f}, val_loss:{val_loss:.4f}, train_acc:{train_acc*100:.4f}, val_acc:{val_acc*100:.4f}')\n",
    "       \n",
    "   \n",
    "        \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d456f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babd2b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
